\documentclass{article}

\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage[hyperref=true, natbib=true, style=numeric, backend=bibtex]{biblatex}
\usepackage{graphicx}
\bibliography{references.bib}

\title{TCP Congestion Algorithms In Datacenters \\
	\vspace{0.3cm}
	{\large LS Project}
}
\date{\today{}}
\author{Luc Gommans, Rick van Gorp}

\begin{document}
\maketitle

\section{Abstract}

In datacenters, multiple tenants share a finite amount of bandwidth. When an
uplink is saturated, packets that overflow available buffer space will be
dropped and have to be retransmitted by the sender. When one tenant chooses to
retransmit faster than others, they have an advantage as they will make it
though more quickly. TCP is a connection-oriented protocol which employs
congestion control algorithms which aim to divide the available bandwidth
equally among network users.

Some congestion control algorithms are more aggressive than others, and it has
been reported that some algorithms cause others to back off excessively.
Tenants may, perhaps inadvertedly, cause unfair bandwidth distributions during
times of congestion.

In this research, we compare five popular algorithms under different network
conditions to see how they influence each other. Among those are the newly
developed BBR and DCTCP. We found TODO


\section{Introduction}

Multitentant datacenters aim to provide services with consistent performance
and reliability. When customers send a lot of data in a short amount of time,
networks might become saturated or even congested, and bandwidth needs to be
distributed fairly among tenants.

Congestion occurs when a buffer on the path to the recipient is full and
packets have to be dropped. TCP uses a congestion control algorithm to avoid
congestion in the network and distribute bandwidth fairly among users. Several
algorithms are available to perform this function. Each has a different behaviour
depending on the network's characteristics.

This report includes a performance and behavior analysis of different TCP congestion control algorithms in a (simulated) multitenant datacenter network. Based on the results, we will recommend datacenters on measures to take to keep providing a reliable environment for customers when multiple congestion control algoritms are active in the network, for example using Quality of Service or discouraging certain congestion control algorithms. The main research question is: {\it How can a datacenter provide a reliable and performant environment for customers when multiple TCP congestion control algorithms are used in the
network?}


\section{Related work}

% Copied from proposal

% todo make this logical
In 2010, the algorithm DCTCP was described in an
article\cite{dctcp-congestion-original} and published as RFC8257 in
2017\cite{dctcp-congestion}. DCTCP is optimized for datacenters and provides a
high-burst tolerance, low latency, and high throughput when the datacenter has
a small part of the buffer available\cite{dctcp-congestion}. In 2016 another
algorithm was proposed: BBR. This is a TCP congestion algorithm created by
Google, which achieves higher bandwidths and lower latencies compared to other
TCP congestion methods\cite{bbr-congestion}. A comparison of BBR with
CUBIC\cite{bbr-congestion-comparison} shows that the BBR node pushes the CUBIC
node away in bandwidth when using small buffers. The BBR node gets more
bandwidth allocated than the CUBIC node.

The TCP congestion control algorithm BBR is discussed in \cite{bbr-congestion}.
The journal contains a performance test of the BBR algorithm and a comparison
between the BBR algorithm and the CUBIC algorithm, where a noticeable
difference in bandwidth allocation is shown when using small buffers.

In \cite{multiple-congestion} a performance analysis of multiple TCP congestion
control algorithms is performed. This research does not include the new
congestion control algorithms we want to analyze and does not include the
datacenter context.

In \cite{dctcp-congestion-original} an algorithm is proposed to avoid
congestion within datacenters. The authors describe an algorithm that maintains
a small buffer occupancy and has an early detection of congestion.

\section{Method}
During this research a performance test of various TCP congestion algorithms will be performed. The most common TCP congestion algorithms will be determined through desk research. The tests will be conducted in a private environment. Based on the results of those tests, recommendations will be given related to the measures a datacenter has to take to keep the traffic equally allocated over customers.

	\subsection{Selecting TCP congestion algorithms}
	In order to select the most common TCP congestion algorithms desk research will be performed. This desk research will show which algorithms are used more often than other algoritms by looking for the default algorithms in Operating Systems. Based on those results a list of TCP congestion algorithms will be composed.

	\subsection{Comparing Performance}
	One scenario will be used to compare the performance of the TCP congestion algorithms:
	\begin{enumerate}
		\item The user owns multiple endpoints in the network. In this case a congestion algorithm is applied at both the client and server side. This scenario applies to multitentant datacenter situations, where a user owns a server in the datacenter network and a node outside of the datacenter network. In this test the user node and random node(s) are at the receiving side and the user server and random server at the sending side. The tests will be performed from a multitenant datacenter perspective.
	\end{enumerate}

	The created testing environment is shown in figure \ref{fig:setup1}. This is a virtual representation of the network as the client node and random node are running on one physical device. The client node and random node are listening each on a different port, which is set by the controller described in section [ENTER SECTION]. The servers are separate physical devices as one of the servers possible runs Windows when a TCP congestion control algorithm is only available for Windows. The software used for the servers and clients is Ubuntu 17.04. If Windows is used, the server will be running Windows 10.

	\begin{figure}[H]
		\centering
  			\includegraphics[scale=0.5]{figs/setup2.png}
  			\caption{Test setup - Scenario 1}
  		\label{fig:setup1}
	\end{figure}

	In scenario 1 the user server uses the same algorithm as the user node and the random server uses the same algorithm as the random node. Tests in this scenario cover behavioral differences between TCP congestion algorithms when a user owns a node in the network of a multitenant datacenter.

	In scenario 1 the TCP congestion algorithm is tested for fairness in sharing bandwidth with other TCP connections using other TCP congestion algorithms. In order to test the fairness of congestion algorithms, the bandwidth allocation per second per algorithm is measured. It can be determined whether a specific TCP congestion algorithm in combination with other TCP congestion algorithms results in the allocation of more or less bandwidth to that specific TCP connection.

The tests conducted on the algorithms have two environment variables: packet loss and delay. Those environment variables are simulated using \texttt{netem}, which is a network emulation functionality in several Linux distributions \cite{linux-netem}. \texttt{netem} is applied to the incoming side of the interfaces of the clients to emulate the same packet loss and delay. Windows, which is tested as a server, does not support \texttt{netem}. Therefore the packet loss and delay are applied to the incoming interfaces of the clients. Packet loss and delay are chosen as the behavior of most TCP algorithms depends on those variables [SRC]. The values for the environment variables are chosen based on measurements performed by Verizon \cite{verizon-latency}.

%\item TCP Window Size measured over time: to determine whether using a specific TCP congestion algorithm in combination with other TCP congestion algorithms results in the allocation of a larger window or a smaller window to a specific TCP connection.
% \item Round Trip Time: to determine the reason of the behaviour of a TCP congestion algorithm in combination with other TCP congestion algorithms.

		\subsubsection{Determining the Packet loss and Delay}
		The dataset of Verizon \cite{verizon-latency} is used to calculate the packet loss and delay used in this research. The dataset includes global information and shows averages per month over the year 2017. The packet loss is calculated by taking the average of the Verizon Business Packet Delivery Statistics for Country Specific Metrics for each country. Those averages were inverted by subtracting the packet delivery percentage from 100, to show the actual packet loss in percentages and not the packet delivery. Based on the ascending sorted values a bar chart was created. This bar chart is shown in figure \ref{fig:packet-loss-chart}

	\begin{figure}[H]
		\centering
  			\includegraphics[scale=0.7]{figs/verizon-packetloss.png}
  			\caption{Average Packet Loss for Country Specific Metrics}
  		\label{fig:packet-loss-chart}
	\end{figure}

The country numbers in figure \ref{fig:packet-loss-chart} are only used to show the distribution of the average packet loss. The maximum packet loss shown in figure \ref{fig:packet-loss-chart} equals 0.6\% and the minimum equals 0\%. However, in case of situations where more packet loss occurs than usual, the double value of the maximum is used: 1.2\%. The values for packet loss in this comparison set-up were determined based on the distribution shown in figure \ref{fig:packet-loss-chart} and the manually set maximum of 1.2\%. The values are shown in table \ref{table:test-packetloss}.

	\begin{table}[H]
		\centering
		\caption{Packet loss percentages used for the comparison setup}
		\begin{tabular}[H]{ | l |}
		\hline
		\textbf{Packet loss} \\
		\hline 0\%. \\
		\hline 0.01\% \\
		\hline 0.1\% \\
		\hline 0.6\% \\
		\hline 1.2\% \\
		\hline
		\end{tabular}
		\label{table:test-packetloss}
	\end{table}

The delay is calculated by taking the average of the Verizon Business Latency Statistics for Country Specific Metrics for each country. Based on the ascending sorted values a bar chart was created. This bar chart is shown in figure \ref{fig:verizon-delay-chart}

	\begin{figure}[H]
		\centering
  			\includegraphics[scale=0.7]{figs/verizon-delay.png}
  			\caption{Average Latency for Country Specific Metrics}
  		\label{fig:verizon-delay-chart}
	\end{figure}

The country numbers in figure \ref{fig:verizon-delay-chart} are only used to show the distribution of the average delays. The maximum delay shown in figure \ref{fig:verizon-delay-chart} equals 290ms and the minimum equals 8ms (both rounded). The values for delay in this comparison set-up were determined based on the distribution shown in figure \ref{fig:verizon-delay-chart}. The values are shown in table \ref{table:test-delay}.

	\begin{table}[H]
		\centering
		\caption{Packet loss percentages used for the comparison setup}
		\begin{tabular}[H]{ | l |}
		\hline
		\textbf{Delay} \\
		\hline  8ms\\
		\hline  64ms  \\
		\hline  120ms  \\
		\hline  176ms \\
		\hline	232ms \\
		\hline  290ms  \\
		\hline
		\end{tabular}
		\label{table:test-delay}
	\end{table}

		\subsubsection{Performing time-based automated tests}
		The tests will be performed using scripts written in Python [\ref{appendix:python}]. The scripts either act as controller or as disciple. The controller is responsible for controlling the tests, by synchronizing the time, exchanging commands to run, modify or stop the tests and gathering data. The disciple is the listener and executes all task commands the controller sends. The commands include getting the hostname of a disciple, setting the TCP congestion algorithm of a disciple, configuring the time of a disciple, starting the data transfer and measurement tools on the disciple and closing the connection with the controller.

		\subsubsection{Testing hardware}
		The specifications of the hardware used for the tests are shown in table \ref{table:spec1}, table \ref{table:spec2} and table \ref{table:spec3}. The switch shown in table \ref{table:spec3} has all router related functionalities, which might impact the network speed, turned off.

		\begin{table}[H]
			\centering
			\caption{Node configurations}
			\begin{tabular}[H]{ | l | l | }
			\hline
			\textbf{Hardware} & \textbf{Specification} \\
			\hline  Model & Dell Optiplex 7010\\
			\hline  CPU & Intel(R) Core(TM) i5-3570S CPU @ 3.10GHz\\
			\hline  Memory & 12GB\\
			\hline  NIC & Intel Corporation 82579LM Gigabit Network Connection (rev 04)\\
			\hline	NIC Driver & e100e 3.2.6-k\\
			\hline
			\end{tabular}
			\label{table:spec1}
		\end{table}

		\begin{table}[H]
			\centering
			\caption{Switch 1 configuration}
			\begin{tabular}[H]{ | l | l | }
			\hline
			\textbf{Hardware} & \textbf{Specification} \\
			\hline  Model & 3Com OfficeConnect Gigabit Switch 16\\
			\hline  Ports & 16x Gigabit Ethernet\\
			\hline
			\end{tabular}
			\label{table:spec2}
		\end{table}

		\begin{table}[H]
			\centering
			\caption{Switch 2 configuration}
			\begin{tabular}[H]{ | l | l | }
			\hline
			\textbf{Hardware} & \textbf{Specification} \\
			\hline  Model & Sitecom Gigabit Router 300N-XR\\
			\hline  Ports & 4x Gigabit Ethernet\\
			\hline
			\end{tabular}
			\label{table:spec3}
		\end{table}

	\subsection{Traffic Management}
	Desk research will be conducted to gather information related to traffic management. The goal is to find a mechanism that is capable of allocating bandwidth equally between customers of a datacenter. This is done by analyzing the results from the algorithm comparison and the initial desk research on traffic management.

\section{Results}
Type some text here.

	\subsection{TCP Congestion Algorithms}
	The most common TCP congestion algorithms are CTCP, CUBIC and BIC. CTCP is the default congestion algorithm in Microsoft Windows Server 2008 and newer \cite{cubic-kernel-version}. CUBIC is the default congestion algorithm in Linux distributions with kernel versions 2.6.19 and higher \cite{cubic-kernel-version}. BIC was the default congestion algorithm in earlier Linux distributions from kernel version 2.6.8 until 2.6.19 \cite{bic-kernel-version} \cite{cubic-kernel-version}. DCTCP and BBR will also be tested as those protocols are new and upcoming---they are included in Linux since 4.9 (December 2016) \cite{linux-bbr}. DCTCP is specifically designed for datacenters \cite{dctcp-congestion} and BBR is a new algorithm meant for general use \cite{bbr-congestion}.

		\subsubsection{Algorithm Characteristics}
		The BBR algorithm depends on parameters $BtlBw$ and $RT_{prop}$ \cite{bbr-congestion}. Those parameters are estimates defined by the BBR algorithm. $BtlBw$ specifies the bandwidth at the slowest link in each direction and $RT_{prop}$ is the round-trip propagation time. $RT_{prop} = RTT - queuing delay - processing delay$, which results in the minimum amount of time for round trip propagation in case there are no queueing or processing delays. Using those two parameters, the Bandwidth Delay Product (BDP) is calculated, which is the maximum possible amount of data being sent in a network: $BDP = BtlBw * RT_{prop}$. As long as the amount of data in transit is less than the $BDP$ and $BtlBw$, the delivery rate is increased. If the amount of data in transit is equal to $BtlBw$ the delivery rate can not go up anymore. The $RTT$ can never be lower than $RT_{prop}$... explain more on delay perhaps?.

		The CTCP algorithm is a combination of a loss-based and delay-based congestion protocol \cite{compound-tcp-congestion}. For the loss-based component, the variable conventional congestion window, $cwnd$, was introduced. For the delay-based component, the variable delay window, $dwnd$, was introduced. The TCP sending window is determined from: $win = min(cwnd + dwnd, awnd)$, where $awnd$ is the window advertised by the receiver. For $cwnd$ on the arrival of an ACK: $cwnd = cwnd + 1/win$. The $dwnd$ variable is initially set to zero and is only set when the congestion avoidance is active. $dwnd$ is calculated differently for several scenarios. If the network path is underutilized: $dwnd(t+1) = dwnd(t) + \alpha*dwnd(t)^{k} - 1$. When the network path is congested: $dwnd(t+1) = dwnd(t) - \eta*diff$. When packet loss occurs: $dwnd(t+1) = dwnd(t)(1-\beta) - cwnd/2$. $k$, $\alpha$, $\eta$ and $\beta$ are tunable parameters for optimization of the algorithm. $\eta$ defines the rapidness of reduction of the window when congestion is detected. $diff$ is calculated from the expected throughput and the actual throughput, which are again calculated from the window size and Round Trip Time (RTT).

		The DCTCP algorithm consists of three components \cite{dctcp-congestion-original}:
		\begin{enumerate}
			\item Simple Marking at the Switch. Based on the marking treshold $K$ a packet is marked with the CE flag. This only occurs when the queue occupancy is greater than K at arrival time. The marking treshold $K$ specifies the minimum value of queue occupancy.
			\item ECN-Echo at receiver. The receiver running the DCTCP algorithm ACKs every packet and sets the ECN-Echo flag only if the packet was marked with CE. The ECN-Echo flag is used to notify the sender of congestion in the network.
			\item Controller at the sender. The sender has an estimate of the fraction of packets that are marked with CE. This is based on the actual fraction of packets that were marked, the weight given to the new estimate compared to the past weight. This results in $\alpha$. $\alpha$ is used to indicate the congestion on a network with a maximum of value 1, indicating a congested network, and a minimum value of 0, indicating no congestion in the network. DCTCP cuts the window size using $\alpha$: $cwnd = cwnd * (1 - \alpha/2)$. This results in a low queue length, but ensuring high throughput.\\
		\end{enumerate}

		The BIC algorithm controls its windows based on the size of windows \cite{bic-tcp-congestion}. It consists of two components:
		\begin{enumerate}
			\item Binary search increase. This algorithm computes the midpoint between the maximum window size $W_{max}$ and current minimum window size $W_{min}$. If packet loss occurs, $W_{max}$ is set to the midpoint. This process repeats until $W_{max}$ and $W_{min}$ are smaller than the minimum increment $S_{min}$.
			\item Additive increase runs as an addition to the binary search increase. If $W_{max} - midpoint > S_{max}$, the window size is increased by $S_{max}$ until $W_{max} - midpoint < S_{max}$. $S_{max}$ is the preset maximum increment.
		\end{enumerate}
In case the current window size becomes greater than $W_{max}$ a slow start strategy is performed to determine a new $W_{max}$: $W_{max} + \alpha * S_{min}$, where $\alpha$ is a multiplier. The multiplier is increased by one until $S_{min} * \alpha >= S_{max}$ or when losses occur. In the case of loss a new value for $W_{max}$ will be set.

		The CUBIC algorithm is a successor of the BIC algorithm \cite{cubic-tcp-congestion}. In case of a loss event $W_{max}$, which is the maximum window size, is registered. This is followed by a multiplicative decrease of the congestion window by factor $\beta$. If an ACK in congestion avoidance is received and the current window size $cwnd$ is less than $W_{max}$, the algorithm uses the concave profile to increase the window size according to the formula: $cwnd = \frac{W(t+RTT)-cwnd}{cwnd}$. If $cwnd$ is larger than $W_{max}$, the convex profile is used to increase the window size according to the same formula as the concave profile. In both formulas $t$ is the time elapsed from the last window reducation.


		\subsubsection{Performance comparison}
		- Includes results of tests.

	\subsection{Traffic Management}
	- Includes what is available for trafic management in a datacenter.
	- Measures to take to equally allocate bandwidth to customers.
		\subsubsection{Quality of Service}




\section{Discussion}


\section{Conclusion}

% For reference, from the proposal:

Our main research question is:
{\it How can a datacenter provide an reliable and performant environment for
customers when multiple TCP congestion control algorithms are used in the
network?}

\vspace{0.5cm}

This main research question divides in five sub-questions:

\begin{enumerate}
	\item What TCP congestion algorithms are most commonly implemented?
	\item How do these TCP congestion algorithms work?
	\item How do these TCP congestion algorithms differ in behavior?
	\item What is the impact on the availability and service for customers when using different, competing congestion algorithms?
	\item What measures could be taken to split bandwidth equally among tenants?
\end{enumerate}


\printbibliography

\appendix
\section{Python}
\label{appendix:python}
SCRIPT

\end{document}

